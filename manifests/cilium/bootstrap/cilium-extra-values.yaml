---
# We manage the entire system's network policies using Cilium
hostFirewall:
  enabled: true

# Enable the Google designed traffic congestion TCP mechanism
bandwidthManager:
  bbr: true
  enabled: true

#bpf:
#  events:
#    drop:
#      enabled: true
#    policyVerdict: true
#    # I should review the trace events to see how impactful they are on the performance of the
#    # network. They may not be worth the threat hunting benefits they provide.
#    #trace:
#    #  enabled: true
#
#  # This is the default, but set it explicitly as cluster IPs should not be available outside the
#  # cluster
#  lbExternalClusterIP: false
#
#  # Which TCP flags trigger notifications when seen for the first time, probably worth tuning this
#  # down...
#  monitorFlags: "all"
#
#  # Reduce use of iptables processing when redirecting Layer 7 traffic to other nodes
#  tproxy: true

# More efficient clock probing
#bpfClockProbe: true

# Only needed in cluster mesh. ID must be unique among all clusters. Name is limited to 32
# alpha-numeric + hyphen and must begin and end with an alpha.
#cluster:
#  id: 0
#  name: "firmament-integration"

# Use SNAT to redirect traffic leaving the cluster. Will likely want this for cleaner auditing even
# if it means an extra network hop... Might instead prefer the masquerade options...
#egressGateway:
#  enabled: true
#enableIPv4Masquerade: true
#enableIPv6Masquerade: true

# Enable transparent network encryption, need to test
#encryption:
#  enabled: true

#priorityClassName: system-node-critical
#resources:
#  requests:
#    cpu: 100m       # Base requirement for packet processing
#    memory: 200Mi   # Minimum for BPF maps and agent
#  limits:
#    # No CPU limit, this is very bursty based on service's usage. We don't want these getting
#    # killed because a service sends a bunch of traffic unexpectedly.
#    memory: 500Mi   # Trigger resets if there is a memory leak, disruptive but more noticable

# Traffic must go through our filters, we do not want to fail open here
#enableXTSocketFallback: false

# All traffic must be explicitly allowed
#policyEnforcementMode: "always"

# While diagnosing the host traffic I don't want to block anything
#policyAuditMode: true

# We're taking over as the kube-proxy, we don't need to wait for anything
#waitForKubeProxy: false

#operator:
#  enabled: true
#  replicas: 1
#  rollOutPods: true

#hubble:
#  enabled: true
#  metrics:
#    enabled:
#      - dns:query # Track DNS metrics
#      - drop  # Track dropped packets for troubleshooting
#      - tcp   # Basic TCP connection metrics
#      - flow  # General network flow information
#      - icmp  # And network diagnostic mode
#      - http  # We'll want to allow matches to selective routes
#    serviceMonitor:
#      enabled: false # Should be enabled in the stable phase
#  #redact: {} # Check out this config object, allows filtering out known sensitive headers from traces...
#  peerService:
#    clusterDomain: "firmament-integration.k8s.grayiron.io"
#  relay:
#    enabled: true
#  tls:
#    enabled: true
#    auto:
#      enabled: true
#      # Warning: This is a temporary certificate and no automated rotation is setup for these
#      # self-signed certs. I intend to replace these in a later stage.
#      method: "cronJob"
