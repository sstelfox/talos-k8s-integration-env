---
cephClusterSpec:
  # Might want this eventually but for now let's leave it off
  dashboard:
    enabled: false

  network:
    connections:
      # This is probably not ncessary for us since we're encrypting all the cluster traffic. By
      # default this is disabled and the CSI driver uses the host network so this would be
      # plaintext content traversing the network.
      encryption:
        enabled: true

      # This may be worth enabling with encryption enabled as compression after encryption is
      # useless. Whether this is a performance increase is going to depend on how loaded our CPUs
      # are to handle the extra compression and how compressible the contents are... Going to guess
      # this will generally not be useful for us...
      compression:
        enabled: false

      # For the use of the more modern protocol since we know our nodes support it. This reduces our
      # attack surface by turning off one of the ports and services and is not needed by any of our
      # clients.
      requireMsgr2: true

      # I would prefer this being an IPv6 native cluster but that still has a ways to go. IPv4 only
      # is also not desirable but there have been issues turning on IPv6 for the cluster so far
      # that need to be resolved before we can fully dual stack the services.
      dualStack: false

  placement:
    # We want the central monitoring ceph nodes to stay on our HA control plane
    mon:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: Exists

    # Our physical server is going to have disks attached to them that we'll want to participate in
    # this cluster as well. We can prioritize those OSDs for control-plane services requiring
    # persistent storage to improve data locality.
    #all:
    #  tolerations:
    #  - key: node-role.kubernetes.io/control-plane
    #    effect: NoSchedule
    #    operator: Exists

  # Crashes may contain sensitive information and should be disabled unless necessary in all
  # production environments. Use of this should be limited to staging and test environments unless
  # a consistent crash is unable to be reproduced outside the production environment.
  crashCollector:
    disable: true

  # This has a built-in log collector on its own with rotation. I'm not sure if this is in addition
  # to allowing the logs to be captured elsewhere. If so I only really need these in that location
  # not all of them.
  # todo(sstelfox): investigate what exactly this does and whether its necessary
  #logCollector:

  # These resource requests start pretty high let's pull out the requests for the big boys for now
  # https://github.com/rook/rook/blob/master/deploy/charts/rook-ceph-cluster/values.yaml
  resources:
    mgr:
      requests: {}
    mon:
      requests: {}
    osd:
      requests: {}

  storage:
    config:
      # The following is only for disks smaller than 100GiB which is the case here for our
      # integration cluster.
      databaseSizeMB: "1024"
      encryptedDevice: "true"

cephBlockPools:
  # This block pool is for semi-persistent services where the loss of data wouldn't be a
  # significant issue such as an on-disk redis cache. This is also useful for ephemeral or trial
  # containers deployed into the cluster. For that reason, this is the default storage class. More
  # permanent services can make the extra effort to use one of the alternative storage types.
  - name: ephemeral-blockpool
    spec:
      failureDomain: host
      replicated:
        size: 1
    storageClass:
      name: ephemeral-block
      enabled: true
      isDefault: true

      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"

      parameters:
        # There are a bunch of features that are off by default as they require kernel 5.4+, but
        # that's below our minimum kernel everywhere. We'll turn on all the extra optimizations on
        # all of our ceph storage classes.
        imageFeatures: layering,fast-diff,object-map,deep-flatten,exclusive-lock

  # For any data we want to ensure stays safe we'll ensure its sufficiently replicated.
  - name: standard-blockpool
    spec:
      failureDomain: host
      replicated:
        size: 3
    storageClass:
      name: standard-block
      enabled: true
      isDefault: false

      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"

      parameters:
        imageFeatures: layering,fast-diff,object-map,deep-flatten,exclusive-lock

cephECBlockPools:
  # We can use erasure coded block pools to achieve higher durability guarantees where necessary
  # but doesn't have as many writes. Very useful for audit log and backup persistence.
  - name: archival-ec-blockpool
    spec:
      metadataPool:
        replicated:
          size: 2
      dataPool:
        failureDomain: osd
        erasureCoded:
          dataChunks: 2
          codingChunks: 1

    parameters:
      imageFeatures: layering,fast-diff,object-map,deep-flatten,exclusive-lock

    storageClass:
      name: archival-block
      enabled: true
      isDefault: false

      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"
