---
# This is an intentionally minimal configuration, just enough to get the CNI up reliably. We'll
# refine and update it later in the bootstrap phase.
ipam:
  mode: kubernetes

kubeProxyReplacement: true

securityContext:
  privileged: true
  capabilities:
    ciliumAgent:
      - CHOWN
      - KILL
      - NET_ADMIN
      - NET_RAW
      - IPC_LOCK
      - SYS_ADMIN
      - SYS_RESOURCE
      - DAC_OVERRIDE
      - FOWNER
      - SETGID
      - SETUID
    cleanCiliumState:
      - NET_ADMIN
      - SYS_ADMIN
      - SYS_RESOURCE

cgroup:
  autoMount:
    enabled: false
  hostRoot: /sys/fs/cgroup

k8sServiceHost: localhost
k8sServicePort: 7445

# Use routing mode as its more efficient on a per-packet level and will be necessary in the larger
# clusters.
routingMode: native
ipv4NativeRoutingCIDR: 10.5.0.0/24

# todo: document these and why we wanted them
autoDirectNodeRoutes: true
directRoutingSkipUnreachable: true

# Couldn't immediately get this working
#ipv6:
#  enabled: true
#ipv6NativeRoutingCIDR: fd42:cafe:face:d00d::/64

# ID must be unique among all clusters. Name is limited to 32 alpha-numeric + hyphen and must begin
# and end with an alpha.
cluster:
  id: 0
  name: "firmament-integration"

hubble:
  enabled: false

# The CNI for the cluster can not be evicted or everything else will be unavailable.
priorityClassName: system-node-critical
resources:
  requests:
    cpu: 100m       # Base requirement for packet processing
    memory: 200Mi   # Minimum for BPF maps and agent
  limits:
    # No CPU limit, this is very bursty based on service's usage. We don't want these getting
    # killed because a service sends a bunch of traffic unexpectedly.
    memory: 500Mi   # Trigger resets if there is a memory leak, disruptive but more noticable
