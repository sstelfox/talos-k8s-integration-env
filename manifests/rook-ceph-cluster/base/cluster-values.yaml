---
cephClusterSpec:
  # Might want this eventually but for now let's leave it off
  dashboard:
    enabled: false

  mgr:
    modules:
      # monitors physical device health and attempts to predict if any storage devices are nearing
      # a failure condition. These health metrics are only collected if health monitoring is
      # enabled on devices. This can be enabled with `ceph device monitoring on`. You can list
      # specific devices with `ceph device ls` and check the life expectency with
      # `ceph device predict-life-expectancy <devid>`. Checking all device health when monitoring
      # is on can be done with `ceph device check-health`.
      #- name: diskprediction_local
      #  enabled: true

      # tracks and monitors health and crash reports in the cluster over the past 24 hours. Periodic
      # reports can be accessed using `ceph insights` but may need to be pruned with
      # `ceph insights prune-health <hours>` periodacally.
      #- name: insight
      #  enabled: true

      # allow viewing the current throughput and IOPS active in the cluster with `ceph iostat`
      #- name: iostat
      #  enabled: true

  network:
    connections:
      # This is probably not ncessary for us since we're encrypting all the cluster traffic. By
      # default this is disabled and the CSI driver uses the host network so this would be
      # plaintext content traversing the network.
      encryption:
        enabled: true

      # This may be worth enabling with encryption enabled as compression after encryption is
      # useless. Whether this is a performance increase is going to depend on how loaded our CPUs
      # are to handle the extra compression and how compressible the contents are... Going to guess
      # this will generally not be useful for us...
      compression:
        enabled: false

      # For the use of the more modern protocol since we know our nodes support it. This reduces our
      # attack surface by turning off one of the ports and services and is not needed by any of our
      # clients.
      requireMsgr2: true

      # I would prefer this being an IPv6 native cluster but that still has a ways to go. IPv4 only
      # is also not desirable but there have been issues turning on IPv6 for the cluster so far
      # that need to be resolved before we can fully dual stack the services.
      dualStack: false

  placement:
    # We want the central monitoring ceph nodes to stay on our HA control plane
    mon:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: Exists

    # Our physical server is going to have disks attached to them that we'll want to participate in
    # this cluster as well. We can prioritize those OSDs for control-plane services requiring
    # persistent storage to improve data locality.
    all:
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
        operator: Exists

    # These aren't technically needed as there aren't disks in the integration cluster that would
    # get picked up by the osd-prepare job but let's be explicit about it. For the physical cluster
    # we'll want these running on the control place as well.
    osd:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: DoesNotExist
    osdprepare:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: DoesNotExist


  # Crashes may contain sensitive information and should be disabled unless necessary in all
  # production environments. Use of this should be limited to staging and test environments unless
  # a consistent crash is unable to be reproduced outside the production environment.
  crashCollector:
    disable: true

  # This has a built-in log collector on its own with rotation. I'm not sure if this is in addition
  # to allowing the logs to be captured elsewhere. If so I only really need these in that location
  # not all of them.
  # todo(sstelfox): investigate what exactly this does and whether its necessary
  #logCollector:

  # These resource requests start pretty high let's pull out the requests for the big boys for now
  # https://github.com/rook/rook/blob/master/deploy/charts/rook-ceph-cluster/values.yaml
  resources:
    mgr:
      requests: {}
    mon:
      requests: {}
    osd:
      requests: {}

  storage:
    config:
      # The following is only for disks smaller than 100GiB which is the case here for our
      # integration cluster.
      databaseSizeMB: "1024"
      encryptedDevice: "true"

cephBlockPools:
  # This block pool is for semi-persistent services where the loss of data wouldn't be a
  # significant issue such as an on-disk redis cache. This is also useful for ephemeral or trial
  # containers deployed into the cluster. For that reason, this is the default storage class. More
  # permanent services can make the extra effort to use one of the alternative storage types.
  - name: ephemeral-blockpool
    spec:
      failureDomain: host
      replicated:
        size: 1
    storageClass:
      name: ephemeral-block
      enabled: true
      isDefault: true

      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"

      parameters:
        # There are a bunch of features that are off by default as they require kernel 5.4+, but
        # that's below our minimum kernel everywhere. We'll turn on all the extra optimizations on
        # all of our ceph storage classes.
        imageFeatures: layering,fast-diff,object-map,deep-flatten,exclusive-lock

  # For any data we want to ensure stays safe we'll ensure its sufficiently replicated.
  - name: standard-blockpool
    spec:
      failureDomain: host
      replicated:
        size: 3
    storageClass:
      name: standard-block
      enabled: true
      isDefault: false

      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"

      parameters:
        imageFeatures: layering,fast-diff,object-map,deep-flatten,exclusive-lock

cephECBlockPools:
  # We can use erasure coded block pools to achieve higher durability guarantees where necessary
  # but doesn't have as many writes. Very useful for audit log and backup persistence.
  - name: archival-ec-blockpool
    spec:
      metadataPool:
        replicated:
          size: 2
      dataPool:
        failureDomain: osd
        erasureCoded:
          dataChunks: 2
          codingChunks: 1

    parameters:
      imageFeatures: layering,fast-diff,object-map,deep-flatten,exclusive-lock

    storageClass:
      name: archival-block
      enabled: true
      isDefault: false

      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"
